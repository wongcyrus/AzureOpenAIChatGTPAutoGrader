{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Grader with Azure OpenAI ChatGPT\n",
    "This notebook can grade students’ assignments automatically by downloading them from Moodle LMS. It will unzip the assignment file from Moodle and create a folder for each student. If a student submits a zip file, it will also unzip it in their folder. The folder should contain either some Docx files or one PDF file. For Docx files, the notebook will extract and merge all the texts into one answer. For PDF files, it will only extract the text from the first page as the answer.\n",
    "\n",
    "The notebook will then use a marking scheme as prompts and let Azure OpenAI ChatGPT evaluate the answer according to the rules. It will also estimate the probability that the answer is copied from the internet or generated by AI.\n",
    "\n",
    "The notebook will use Azure OpenAI text-embedding-3-large to get the embedding of the answer. It will then use K-means clustering to group the answers based on their embeddings and show the teachers the different types of answers. It will also perform PCA on the embeddings and plot the first three principal components in 3D. This will help the teachers see how similar or different the answers are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pypandoc docx2txt PyPDF2 openpyxl python-dotenv openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken ipywidgets seaborn ipympl\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file and return the content\n",
    "def read_text_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "def write_text_to_file(path, content):\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all submissions to a tmp folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the zipfile module\n",
    "from zipfile import ZipFile\n",
    "# Create a zip file object using ZipFile class\n",
    "with ZipFile(\"data/submission.zip\", \"r\") as zip_obj:\n",
    "    # Extract all the files into a directory\n",
    "    zip_obj.extractall(\"tmp/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os module\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to list\n",
    "temp_path = \"tmp/\"\n",
    "\n",
    "def is_folder_contains_file(folder_path, extension): \n",
    "    # Get a list of all files and directories in the path \n",
    "    names = os.listdir(folder_path) \n",
    "    for name in names: \n",
    "        if name.lower().endswith(extension.lower()): \n",
    "            return True \n",
    "    return False   \n",
    "    \n",
    "# Get a list of all files and directories in the path\n",
    "def get_submissions_df(path):\n",
    "    assignment_folders = []\n",
    "    names = os.listdir(path)\n",
    "    # Loop through the list\n",
    "    for name in names:\n",
    "        # Join the path and the name\n",
    "        full_path = os.path.join(path, name)\n",
    "        # Check if it is a directory\n",
    "        if os.path.isdir(full_path):\n",
    "            # Print the directory name\n",
    "            assignment_folders.append({\n",
    "                \"Student\": name.split(\"_\")[0],\n",
    "                \"Path\": full_path,\n",
    "                \"ContainsDocxFile\": is_folder_contains_file(full_path, \".docx\"),                \n",
    "                \"ContainsPdfFile\": is_folder_contains_file(full_path, \".pdf\"),\n",
    "                \"ContainsZipFile\": is_folder_contains_file(full_path, \".zip\")\n",
    "                })\n",
    "    df = pd.DataFrame([p for p in assignment_folders])\n",
    "    return df\n",
    "df = get_submissions_df(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that all the files submitted are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_not_contains_any_expected_files(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == False) & (df[\"ContainsPdfFile\"] == False) & (df[\"ContainsZipFile\"] == False)]\n",
    "filter_df_by_not_contains_any_expected_files(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def flatten(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory, topdown=False):\n",
    "        for filename in filenames:\n",
    "            i = 0\n",
    "            source = os.path.join(dirpath, filename)\n",
    "            target = os.path.join(directory, filename)\n",
    "\n",
    "            while os.path.exists(target):\n",
    "                i += 1\n",
    "                file_parts = os.path.splitext(os.path.basename(filename))\n",
    "\n",
    "                target = os.path.join(\n",
    "                    directory,\n",
    "                    file_parts[0] + \"_\" + str(i) + file_parts[1],\n",
    "                )\n",
    "\n",
    "            shutil.move(source, target)\n",
    "\n",
    "            print(\"Moved \", source, \" to \", target)\n",
    "\n",
    "        if dirpath != directory:\n",
    "            os.rmdir(dirpath)\n",
    "            print(\"Deleted \", dirpath)\n",
    "\n",
    "def get_first_file_path(path, ext):\n",
    "    names = os.listdir(path)\n",
    "    for name in names:\n",
    "        if name.endswith(ext):\n",
    "            return os.path.join(path, name)\n",
    "\n",
    "def extract_zip_file_in_place(path):\n",
    "    zip_path = get_first_file_path(path, \".zip\")\n",
    "    print(zip_path)\n",
    "    import zipfile\n",
    "    # Create a zip file object using ZipFile class\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_obj:\n",
    "        # Extract all the files into a directory\n",
    "        zip_obj.extractall(path)\n",
    "    flatten(path) \n",
    "\n",
    "\n",
    "def filter_df_by_contains_zip_file(df):\n",
    "    return df[(df[\"ContainsZipFile\"] == True)]\n",
    "\n",
    "paths = filter_df_by_contains_zip_file(df)[\"Path\"].values\n",
    "for path in paths:\n",
    "    extract_zip_file_in_place(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_submissions_df(temp_path)\n",
    "## check all rows contains Docx or PDF file\n",
    "def filter_df_by_contains_docx_or_pdf_file(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == True) | (df[\"ContainsPdfFile\"] == True)]\n",
    "\n",
    "filter_df_by_contains_docx_or_pdf_file(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_contains_docx(df):\n",
    "    return df[(df[\"ContainsDocxFile\"] == True)]\n",
    "words_df = filter_df_by_contains_docx(df)\n",
    "paths = words_df[\"Path\"].values\n",
    "\n",
    "def get_all_docx_files(path):\n",
    "    import glob\n",
    "    return glob.glob(path + \"/*.docx\")\n",
    "\n",
    "import docx2txt\n",
    "from functools import reduce\n",
    "\n",
    "students_words_files = list(map(get_all_docx_files, paths)) # List of lists of word files\n",
    "\n",
    "file_contents =[];\n",
    "for word_files in students_words_files:  \n",
    "    file_contents.append(reduce(lambda x, y: x + y, map(lambda f: docx2txt.process(f), word_files), \"\\n\\n\"))\n",
    "# reduce(map(lambda f: docx2txt.process(f), word_files), lambda x, y: x + y, \"\")\n",
    "words_df.loc[:, \"Sources\"] = students_words_files\n",
    "words_df.loc[:, \"Answers\"] = file_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_contains_pdf(df):\n",
    "    return df[(df[\"ContainsPdfFile\"] == True)]\n",
    "pdfs_df = filter_df_by_contains_pdf(df)\n",
    "paths = pdfs_df[\"Path\"].values\n",
    "\n",
    "def get_add_pdf_files(path):\n",
    "    import glob\n",
    "    return glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "import PyPDF2\n",
    "from functools import reduce\n",
    "\n",
    "def convert_pdf_all_pages_to_txt(path):\n",
    "    pdfFileObj = open(path, 'rb')\n",
    "    reader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    num_pages = len(reader.pages)\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    while count < num_pages:\n",
    "        pageObj = reader.pages[count]\n",
    "        count += 1\n",
    "        text += pageObj.extract_text()\n",
    "        text += \"\\n\\n\"\n",
    "    return text\n",
    "\n",
    "students_pdf_files = list(map(get_add_pdf_files, paths)) # List of lists of word files\n",
    "\n",
    "file_contents =[];\n",
    "for pdf_files in students_pdf_files:\n",
    "    file_contents.append(reduce(lambda x, y: x + y, map(convert_pdf_all_pages_to_txt, pdf_files), \"\\n\\n\"))\n",
    "\n",
    "pdfs_df.loc[:, \"Sources\"] = students_pdf_files\n",
    "pdfs_df.loc[:, \"Answers\"] = file_contents\n",
    "pdfs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two dataframes into one and export to excel\n",
    "df_answers = pd.concat([words_df, pdfs_df])\n",
    "df_answers.to_excel(\"data/answers.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading students’ responses using Azure OpenAI ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\")\n",
    ")\n",
    "\n",
    "def mark_result(marks, copyFromInternet, generativeAI, manualReview, comments):\n",
    "    return {\n",
    "        \"marks\": marks,\n",
    "        \"copyFromInternet\": copyFromInternet,\n",
    "        \"generativeAI\": generativeAI,    \n",
    "        \"manualReview\": manualReview,\n",
    "        \"comments\": comments    \n",
    "    }\n",
    "\n",
    "\n",
    "def get_json_chatGpt(student, prompt):    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a teaching assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "        max_tokens=1600,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"mark_result\",\n",
    "                    \"description\": \"Return the grading result for the student answer.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"marks\": {\"type\": \"integer\"},\n",
    "                            \"copyFromInternet\": {\"type\": \"number\"},\n",
    "                            \"generativeAI\": {\"type\": \"number\"},\n",
    "                            \"manualReview\": {\"type\": \"boolean\"},\n",
    "                            \"comments\": {\"type\": \"string\"}\n",
    "                        },\n",
    "                        \"required\": [\"marks\", \"copyFromInternet\", \"generativeAI\", \"manualReview\", \"comments\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"mark_result\"}}\n",
    "    )\n",
    "    # Extract the tool call result from the response\n",
    "    tool_calls = response.choices[0].message.tool_calls\n",
    "    if tool_calls and tool_calls[0].function and tool_calls[0].function.arguments:\n",
    "        response.choices[0].message.content = tool_calls[0].function.arguments\n",
    "    else:\n",
    "        response.choices[0].message.content = \"{}\"\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    # Convert response to JSON for saving to file\n",
    "    response_json = {\n",
    "        \"id\": response.id,\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"message\": {\n",
    "                    \"content\": response.choices[0].message.content,\n",
    "                    \"role\": response.choices[0].message.role\n",
    "                },\n",
    "                \"index\": response.choices[0].index,\n",
    "                \"finish_reason\": response.choices[0].finish_reason\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    write_text_to_file(f\"tmp/{student}.json\", json.dumps(response_json))\n",
    "    tokens = response.usage.total_tokens\n",
    "    return json.loads(response.choices[0].message.content), tokens\n",
    "\n",
    "def grade_answer(student, student_answer, marking_scheme):    \n",
    "    prompt=marking_scheme.replace(\"<ANSWER></ANSWER>\", student_answer)\n",
    "    retry = 0; \n",
    "    while True:\n",
    "        try:\n",
    "            content, tokens = get_json_chatGpt(student,prompt)\n",
    "            break             \n",
    "        except Exception as e:            \n",
    "            if retry < 2:                \n",
    "                retry += 1\n",
    "                print(e)\n",
    "                print(\"retry: \" + str(retry))\n",
    "                continue            \n",
    "            return 0, \"Error\", 0, 0, True, 0, True\n",
    "    marks = content['marks']\n",
    "    comments = content['comments']       \n",
    "    copyFromInternet = content['copyFromInternet']\n",
    "    generativeAI = content['generativeAI']        \n",
    "    manualReview = content['manualReview']     \n",
    "    return marks, comments, copyFromInternet, generativeAI, manualReview, tokens, False    \n",
    "\n",
    "def grade_answers(df_answers, marking_scheme):\n",
    "    for index, row in df_answers.iterrows():      \n",
    "        student = row[\"Student\"]\n",
    "        print(student)\n",
    "        answer = row[\"Answers\"]\n",
    "       \n",
    "        marks, comments, copyFromInternet, generativeAI, manualReview, tokens, error = grade_answer(student, answer, marking_scheme)\n",
    "        df_answers.loc[index, \"Marks\"] = marks\n",
    "        df_answers.loc[index, \"Comments\"] = comments\n",
    "        df_answers.loc[index, \"CopyFromInternet\"] = copyFromInternet\n",
    "        df_answers.loc[index, \"GenerativeAI\"] = generativeAI\n",
    "        df_answers.loc[index, \"ChatGptTokens\"] = tokens     \n",
    "        df_answers.loc[index, \"ManualReview\"] = manualReview\n",
    "        df_answers.loc[index, \"Error\"] = error\n",
    "    return df_answers\n",
    "\n",
    "marking_scheme = read_text_file(\"marking_scheme.txt\")\n",
    "\n",
    "# get second row answer for df_answers\n",
    "student = df_answers.iloc[[2]][\"Student\"].values[0]\n",
    "student_answer = df_answers.iloc[[2]][\"Answers\"].values[0]\n",
    "# print(student_answer)\n",
    "# print(marking_scheme)\n",
    "# grade_answer(student, student_answer, marking_scheme)\n",
    "\n",
    "df_marked = grade_answers(df_answers, marking_scheme)\n",
    "df_marked.to_excel(\"data/marks.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "model_name = \"text-embedding-3-large\"\n",
    "deployment = \"text-embedding-3-large\"\n",
    "\n",
    "api_version = \"2024-02-01\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    client = AzureOpenAI(\n",
    "        api_version=api_version,\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    )\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model_name\n",
    "    )    \n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked = pd.read_excel(\"data/marks.xlsx\") \n",
    "df_Answers = df_marked[['Student','Answers']]\n",
    "df_Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning by removing redundant whitespace and cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()    \n",
    "    return s if len(s) > 0 else \"Do nothing\"\n",
    "\n",
    "df_Answers['Answers']= df_Answers[\"Answers\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any answers that are too long for the token limit (8192 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df_Answers['n_tokens'] = df_Answers[\"Answers\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_Answers = df_Answers[df_Answers.n_tokens<8192]\n",
    "len(df_Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Answers['ada_v2'] = df_Answers[\"Answers\"].apply(lambda x : get_embedding(x)) \n",
    "# df_Answers.set_index( ['Student'], inplace = True)\n",
    "# engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df_Answers.to_excel(\"data/embeddings.xlsx\", index=True)\n",
    "df_Answers.apply(lambda x : write_text_to_file(f\"tmp/embeddings_{x.Student}.json\", json.dumps(x.ada_v2)), axis=1)\n",
    "df_Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering based on the Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df_embeddings = df_Answers.copy()\n",
    "df_embeddings = pd.read_excel(\"data/embeddings.xlsx\") \n",
    "def reload_embeddings(student):\n",
    "    return list(json.loads(read_text_file(f\"tmp/embeddings_{student.Student}.json\")))\n",
    "df_embeddings[\"ada_v2\"] = df_embeddings.apply(lambda s : reload_embeddings(s), axis=1)\n",
    "df_embeddings.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_embeddings.set_index( ['Student'], inplace = True)\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "matrix = np.array(df_embeddings[\"ada_v2\"].to_list())\n",
    "n_clusters = 7\n",
    "kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42, n_init='auto')\n",
    "kmeans.fit(matrix)\n",
    "labels = kmeans.labels_\n",
    "df_embeddings[\"Cluster\"] = labels \n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a larger figure with more room for margins\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Set up t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate=200)\n",
    "vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "x = [x for x,y in vis_dims2]\n",
    "y = [y for x,y in vis_dims2]\n",
    "\n",
    "palette = sns.color_palette(\"inferno\", n_clusters).as_hex()  # Match palette size to n_clusters\n",
    "\n",
    "# Create the scatter plot with all clusters\n",
    "for category, color in enumerate(palette):\n",
    "    # Get indices of points in this cluster\n",
    "    cluster_indices = np.where(df_embeddings[\"Cluster\"] == category)[0]\n",
    "    \n",
    "    # Only plot if there are points in this cluster\n",
    "    if len(cluster_indices) > 0:\n",
    "        # Extract x and y values for this cluster\n",
    "        xs = np.array(x)[cluster_indices]\n",
    "        ys = np.array(y)[cluster_indices]\n",
    "        \n",
    "        # Plot the individual points with consistent size\n",
    "        s_value = 30  # Size for each point\n",
    "        plt.scatter(xs, ys, s=s_value, color=color, alpha=0.3)\n",
    "        \n",
    "        # Calculate and plot the cluster center\n",
    "        avg_x = np.mean(xs)\n",
    "        avg_y = np.mean(ys)\n",
    "        plt.scatter([avg_x], [avg_y], marker='x', color=color, s=200)\n",
    "\n",
    "# Add title and labels with explicit fontsize\n",
    "plt.title(\"Embeddings visualized using t-SNE\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE dimension 2\", fontsize=12)\n",
    "\n",
    "# Add padding around the plot instead of using tight_layout\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the final result cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marked_tmp=pd.read_excel(\"data/marks.xlsx\") \n",
    "df_embeddings_tmp=df_embeddings.copy()\n",
    "df_marked_tmp.set_index( ['Student'], inplace = True)\n",
    "# df_embeddings_tmp.set_index( ['Student'], inplace = True)\n",
    "df_final = pd.merge(df_marked_tmp, df_embeddings_tmp[[\"n_tokens\",\"ada_v2\",\"Cluster\"]], how='left', left_index=True, right_index=True)\n",
    "\n",
    "cols = ['Marks', 'Comments', 'Answers','CopyFromInternet','GenerativeAI','ChatGptTokens','ManualReview','Error','Cluster']\n",
    "\n",
    "df_final= df_final[cols]\n",
    "df_final.to_excel(\"data/final.xlsx\", index=True)\n",
    "df_final.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_df = df_embeddings.copy()\n",
    "matrix = pca_df[\"ada_v2\"].to_list()\n",
    "pca = PCA(n_components=3)\n",
    "vis_dims = pca.fit_transform(matrix)\n",
    "pca_df[\"embed_vis\"] = vis_dims.tolist()\n",
    "pca_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the total variance each principal component captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sum(pca.explained_variance_ratio_)*100)+\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the Change in Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nums = np.arange(14)\n",
    "\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(matrix)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4,2),dpi=150)\n",
    "plt.grid()\n",
    "plt.plot(nums,var_ratio,marker='o')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.title('n_components vs. Explained Variance Ratio')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "clusters = pca_df[\"Cluster\"].to_list()\n",
    "\n",
    "# Plot each sample category individually such that we can set label name.\n",
    "for i, clusterId in enumerate(clusters):\n",
    "    sub_matrix = np.array(pca_df[pca_df[\"Cluster\"] == clusterId][\"embed_vis\"].to_list())\n",
    "    \n",
    "    x=sub_matrix[:, 0]\n",
    "    y=sub_matrix[:, 1]\n",
    "    z=sub_matrix[:, 2]\n",
    "    colors = [cmap(i/len(clusters))] * len(sub_matrix)\n",
    "    ax.scatter(x, y, zs=z, zdir='z', c=colors, label=clusterId)\n",
    "\n",
    "    students = pca_df[pca_df[\"Cluster\"] == clusterId].index.values.tolist()\n",
    "    for i, txt in enumerate(students):\n",
    "        ax.text(x[i], y[i], z[i], txt, size=8, zorder=1, color='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "# ax.legend(bbox_to_anchor=(1.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
